{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "module_path = os.path.abspath(os.path.join(os.pardir, os.pardir))\n",
    "if module_path not in sys.path:    \n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:    \n",
    "    sys.path.append(module_path)\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import platform\n",
    "from sleep_stage_config import Config\n",
    "from utilities.utils import *\n",
    "from sklearn.preprocessing import Normalizer\n",
    "#sns.set(style='whitegrid', rc={'axes.facecolor': '#EFF2F7'})\n",
    "import hrvanalysis as hrvana\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This tutorial will explain the feature extraction pipeline for activity counts and heart rate variabilities in details\n",
    "This tutorial only focuses on the pipline of feature extraction for each modality. We didn't align the actigraphy data and RR interval data by sleep epochs. You could find detailed information about aligning activity counts and RR intervals in `align_actigraphy_rri.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_path = r\"\\Dataset\\MESA\\actigraphy\"   # the directory is the actigraphy data you downloaded from MESA\n",
    "acc_feature_output_path = r\"\\tmp\\sleep\\act_features\" # the directory is the actigraphy feature you want to save\n",
    "hr_path = r\"\\Dataset\\MESA\\annotations-rpoints\" # the directory is the R-points you downloaded from MESA\n",
    "admin_file_path = r\"\\Dataset\\MESA\\mesa-sleep-dataset-0.3.0.csv\" # the directory is the admin data you downloaded from MESA\n",
    "hrv_feature_output_path = r\"\\tmp\\sleep\\hrv_features\" # the directory is the HRV features you want to save\n",
    "standarize_feature = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "admin_df = pd.read_csv(admin_file_path)\n",
    "total_subjects_list = admin_df['mesaid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_acc_files = os.listdir(acc_path)\n",
    "all_hr_files = os.listdir(hr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1966"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_hr_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 The pipeline of feature extraction for heart rate variability in details\n",
    "Note: \n",
    "In our paper, we only used a single window length of 30s to extract the features related to RR-interval. This method will diminish physiological meaning of these features but yield a better classification outcome. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrv_win = 0 # 0 means we use one sleep epoch as the window length (every 30s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mesa-sleep-0001-rpoint.csv'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_hr_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only extract the HRV features if the subject is in the valid subject list\n",
    "for PID in tqdm(total_subjects_list):\n",
    "    mesa_id = \"%04d\" % PID\n",
    "    hr_inlist_idx = [s for s in all_hr_files if mesa_id in s]\n",
    "#     print(\"processing pid: %s\" % os.path.basename(file).split(\"-\")[2])\n",
    "#     mesa_id = os.path.basename(file).split(\"-\")[2]\n",
    "    if len(hr_inlist_idx) > 0: # the pid is in the file list.\n",
    "        hr_file_idx = all_hr_files.index(''.join(hr_inlist_idx))\n",
    "        hr_df = pd.read_csv(os.path.join(hr_path, all_hr_files[hr_file_idx]))\n",
    "\n",
    "        hr_df = hr_df[hr_df['TPoint'] > 0]\n",
    "        hr_df['RR Intervals'] = hr_df['seconds'].diff() * 1000\n",
    "        hr_df.loc[0, 'RR Intervals']=hr_df.loc[0]['seconds'] * 1000 # make sure the data export from PSG software doesn't contain Nan\n",
    "        clean_rri = hr_df['RR Intervals'].values\n",
    "        clean_rri = hrvana.remove_outliers(rr_intervals=clean_rri, low_rri=300, high_rri=2000)\n",
    "        clean_rri = hrvana.interpolate_nan_values(rr_intervals=clean_rri, interpolation_method=\"linear\")\n",
    "        clean_rri = hrvana.remove_ectopic_beats(rr_intervals=clean_rri, method=\"malik\")\n",
    "        clean_rri = hrvana.interpolate_nan_values(rr_intervals=clean_rri)\n",
    "        hr_df[\"RR Intervals\"] = clean_rri\n",
    "        # calculate the Heart Rate\n",
    "        hr_df['HR'] = np.round((60000.0 / hr_df['RR Intervals']), 0)\n",
    "\n",
    "        # filter RRI if the RR intervals doesn't contain at least 4 valid beats\n",
    "        t1 = hr_df.epoch.value_counts().reset_index().rename({'index': 'epoch_idx', 'epoch': 'count'}, axis=1)\n",
    "        invalid_idx = set(t1[t1['count'] < 3]['epoch_idx'].values)\n",
    "        del t1\n",
    "        hr_df = hr_df[~hr_df['epoch'].isin(list(invalid_idx))]\n",
    "        feature_list = []\n",
    "        # go through all sleep epochs and extract HRV features from them\n",
    "        for index, hr_epoch_idx in enumerate(hr_df['epoch'].unique()):\n",
    "            gt_label = hr_df[hr_df['epoch'] == hr_epoch_idx][\"stage\"].values[0]  # in MESA r-points files, the stage is annotated by the sleep experts\n",
    "            if hrv_win != 0:  # if you want get the features from a 5 min window, please set hrv_win = 10, as per sleep epoch is 30s\n",
    "                offset = int(np.floor(hrv_win/2))\n",
    "                tmp_hr_df = hr_df[hr_df['epoch'].isin(np.arange(hr_epoch_idx-offset, hr_epoch_idx+offset))]\n",
    "            else:\n",
    "                tmp_hr_df = hr_df[hr_df['epoch'] == hr_epoch_idx]\n",
    "            try:  # check to see if the first time stamp is empty\n",
    "                start_sec = float(tmp_hr_df['seconds'].head(1) * 1.0)\n",
    "            except Exception as ee:\n",
    "                print(\"Exception %s, source dataset: %s\" % (ee, tmp_hr_df['seconds'].head(1)))\n",
    "            # calculate the HRV features for each epoch\n",
    "            rr_epoch = tmp_hr_df['RR Intervals'].values\n",
    "            all_hr_features = {}\n",
    "            try:\n",
    "                all_hr_features.update(hrvana.get_time_domain_features(rr_epoch))\n",
    "            except Exception as ee:\n",
    "                print(\"processed time domain features with error message: {}\".format(str(ee)))\n",
    "            try:\n",
    "                all_hr_features.update(hrvana.get_frequency_domain_features(rr_epoch))\n",
    "            except Exception as ee:\n",
    "                print(\"processed frequency domain features with error message: {}\".format(str(ee)))\n",
    "            try:\n",
    "                all_hr_features.update(hrvana.get_poincare_plot_features(rr_epoch))\n",
    "            except Exception as ee:\n",
    "                print(\"processed poincare features: {} with error message\".format(str(ee)))\n",
    "            try:\n",
    "                all_hr_features.update(hrvana.get_csi_cvi_features(rr_epoch))\n",
    "            except Exception as ee:        \n",
    "                print(\"processed csi cvi domain features: {} with error message\".format(str(ee)))\n",
    "            try:\n",
    "                all_hr_features.update(hrvana.get_geometrical_features(rr_epoch))\n",
    "            except Exception as ee:\n",
    "                print(\"processed geometrical features: {} with error message\".format(str(ee)))\n",
    "\n",
    "            all_hr_features.update({'stages': gt_label\n",
    "                                    , 'mesaid': str(mesa_id)                            \n",
    "                                    , 'epoch': hr_epoch_idx\n",
    "                                    #, 'index': index\n",
    "                                    })\n",
    "            feature_list.append(all_hr_features)\n",
    "        hrv_feature_df = pd.DataFrame(feature_list)\n",
    "        hrv_feature_df.to_csv(os.path.join(hrv_feature_output_path, (mesa_id + '_hrv_features.csv')), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part-2 The pipeline of feature extraction for activity counts in details\n",
    "Note: \n",
    "\n",
    "For the actigraphy based sliding window method, the `get_statistic_feature` function will calculate statistic features based on two window-centring methods. The centred window and backwards-looking window. The dataframe passed in as the function's argument will be expanded to include the calculated features. In this tutorial, we show an example of using 20 sleep epochs as the length of the window. In MESA actigraphy, the 370 calculated features will be appended after column *daybynoon* the calculated actigraphy features can be found in `acc_feature_output_path`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for PID in tqdm(total_subjects_list):\n",
    "    mesa_id = \"%04d\" % PID\n",
    "    # filter Acc and HR based on the overlap records\n",
    "    print('*' * 100)\n",
    "    print(\"Processing subject %s dataset\" % mesa_id)\n",
    "    acc_inlist_idx = [s for s in all_acc_files if mesa_id in s]\n",
    "    feature_list = []\n",
    "    if len(acc_inlist_idx) > 0: # the pid is in the file list.\n",
    "        # get the raw dataset file index\n",
    "        acc_file_idx = all_acc_files.index(''.join(acc_inlist_idx))\n",
    "        # load Acc and HR into Pandas\n",
    "        acc_df = pd.read_csv(os.path.join(acc_path, all_acc_files[acc_file_idx]))\n",
    "        # filter ACC, as per MESA website recommanded.\n",
    "        acc_df = acc_df[acc_df['interval'] != 'EXCLUDED']\n",
    "        #combined_pd = combined_pd.reset_index(drop=True)\n",
    "        acc_df['timestamp'] = pd.to_datetime(acc_df['linetime'])\n",
    "        acc_df['base_time'] = pd.to_datetime('00:00:00')\n",
    "        acc_df['seconds'] = (acc_df['timestamp'] - acc_df['base_time'])\n",
    "        acc_df['seconds'] = acc_df['seconds'].dt.seconds\n",
    "        acc_df.drop(['timestamp', 'base_time'], axis=1, inplace=True)\n",
    "        featnames = get_statistic_feature(acc_df, column_name=\"activity\", windows_size=20)\n",
    "\n",
    "        list_size_chk = np.array(acc_df[['marker', 'activity']].values.tolist())\n",
    "        # check whether the activity is empty\n",
    "        if len(list_size_chk.shape) < 2:\n",
    "            print(\n",
    "                \"File {f_name} doesn't meet dimension requirement, it's size is {wrong_dim}\".format(\n",
    "                    f_name=all_acc_files[acc_file_idx], wrong_dim=list_size_chk.shape)\n",
    "            )\n",
    "        else:\n",
    "            acc_df = acc_df.fillna(acc_df.median()) # fill the missing data with their median value which is resistant to outliters \n",
    "            # standardise and normalise the df\n",
    "            feature_list = acc_df.columns.to_list() \n",
    "            std_feature = [x for x in feature_list if x not in ['two_stages', 'seconds', 'activity', 'interval', 'wake', 'linetime', 'mesaid', 'stages', 'line']]\n",
    "            if standarize_feature:\n",
    "                standardize_df_given_feature(acc_df, std_feature, df_name='acc_df', simple_method=False)\n",
    "            acc_df.to_csv(os.path.join(acc_feature_output_path, (mesa_id + '_act_features.csv')), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
